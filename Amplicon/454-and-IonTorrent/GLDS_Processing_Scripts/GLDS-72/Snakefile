############################################################################################
## Snakefile for GeneLab's processing of non-Illumina amplicon data                       ##
## https://genelab-data.ndc.nasa.gov/genelab/accession/GLDS-72/                           ##
##                                                                                        ##
## This file as written expects to be executed within the "processing_info/" directory    ##
## with the raw starting fastq files present in the "../Raw_Data/" directory              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os

########################################
############# General Info #############
########################################

"""
Expected to be run in the following conda environment (or with specified programs versions accessible):

# conda create -y -n GL-454-Ion-Torrent-amplicon -c conda-forge -c bioconda -c defaults fastqc=0.11.9 \
#              multiqc=1.9 cutadapt=2.10 bbmap=38.86 r-base=3.6.3 bioconductor-decipher=2.12.0 \
#              bioconductor-biomformat=1.12.0 vsearch=2.15.1 snakemake=5.5.4

# conda activate GL-454-Ion-Torrent-amplicon
"""

########################################
######## Setting some variables ########
########################################

  # current GLDS number
curr_GLDS = "GLDS-72"

  ## info for cutadapt ##
  # Cutadapt command as written currently expects "standard" orientation (as much as that's possible with primers and amplicon data),
  # command run currently is as such: cutadapt -g {F_primer} -a {R_primer} -o {output} {input}
  # but different datasets may need to have the command adjusted accordingly below in the cutadapt rule if needed (see https://cutadapt.readthedocs.io/en/stable/guide.html for help)

  # primers
F_primer = "AGAGTTTGATCCTGGCTCAG"
R_primer = "GCTGCCTCCCGTAGGAGT"

  # single column file holding unique portion of sample names
sample_IDs_file = "unique-sample-IDs.txt"

  # useful prefixes and suffixes for filename structure
  # filename prefix (what comes before the unique portion of the name that is provided in the "unique-sample-IDs.txt" file)
filename_prefix = ""
  # filename suffixes (what comes after the unique portion of the names, for R1 and R2 files)
raw_filename_suffix = "_raw.fastq.gz"
primer_trimmed_filename_suffix = "_trimmed.fastq.gz"
filtered_filename_suffix = "_filtered.fastq.gz"
raw_fastqc_suffix = "_raw_fastqc.zip"
filtered_fastqc_suffix = "_filtered_fastqc.zip"

  # directories (all relative to processing directory)
raw_reads_dir = "../Raw_Data/"
fastqc_out_dir = "../FastQC_Outputs/"
trimmed_reads_dir = "../Trimmed_Sequence_Data/"
filtered_reads_dir = "../Filtered_Sequence_Data/"
final_outputs_dir = "../Final_Outputs/"

needed_dirs = [fastqc_out_dir, trimmed_reads_dir, filtered_reads_dir, final_outputs_dir]

  # number of threads to use PER snakemake job started (that's determined by the -j parameter passed to the snakemake call)
    # passed to fastqc (since only fastqc here, 2 is fine as only 2 files are passed to each call; functions in R are currently on autodetect)
num_threads = 2

########################################
#### Reading samples file into list ####
########################################

sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in needed_dirs:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}" + filtered_filename_suffix, ID = sample_ID_list),
        expand(trimmed_reads_dir + filename_prefix + "{ID}" + primer_trimmed_filename_suffix, ID = sample_ID_list),
        trimmed_reads_dir + "cutadapt.log",
        trimmed_reads_dir + "trimmed-read-counts.tsv",
        filtered_reads_dir + "bbduk.log",
        filtered_reads_dir + "filtered-read-counts.tsv",
        final_outputs_dir + "taxonomy.tsv",
        final_outputs_dir + "taxonomy-and-counts.biom.zip",
        final_outputs_dir + "OTUs.fasta",
        final_outputs_dir + "read-count-tracking.tsv",
        final_outputs_dir + "counts.tsv",
        final_outputs_dir + "taxonomy-and-counts.tsv",
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip",
        fastqc_out_dir + "filtered_multiqc_report.html.zip",
        fastqc_out_dir + "filtered_multiqc_data.zip"
    shell:
        """
        # copying log file to store with processing info
        cp .snakemake/log/$(ls -t .snakemake/log/ | head -n 1) snakemake-run.log
        """


rule cutadapt:
    input:
        raw_reads_dir + filename_prefix + "{ID}" + raw_filename_suffix
    output:
        trimmed_reads = trimmed_reads_dir + filename_prefix + "{ID}" + primer_trimmed_filename_suffix,
        trim_counts = trimmed_reads_dir + "{ID}-trimmed-counts.tsv"
    log:
        trimmed_reads_dir + "{ID}-cutadapt.log"
    shell:
        """
        cutadapt -g {F_primer} -a {R_primer} -o {output.trimmed_reads} {input} > {log} 2>&1
        paste <( printf "{wildcards.ID}" ) <( grep "Total reads processed:" {log} | tr -s " " "\t" | cut -f 4 | tr -d "," ) <( grep "Reads written (passing filters):" {log} | tr -s " " "\t" | cut -f 5 | tr -d "," ) > {output.trim_counts}
        """


rule combine_cutadapt_logs_and_summarize:
    input:
        counts = expand(trimmed_reads_dir + "{ID}-trimmed-counts.tsv", ID = sample_ID_list),
        logs = expand(trimmed_reads_dir + "{ID}-cutadapt.log", ID = sample_ID_list)
    output:
        combined_log = trimmed_reads_dir + "cutadapt.log",
        combined_counts = trimmed_reads_dir + "trimmed-read-counts.tsv"
    shell:
        """
        cat {input.logs} > {output.combined_log}
        rm {input.logs}
        
        cat <( printf "sample\traw_reads\tcutadapt_trimmed\n" ) <( cat {input.counts} ) > {output.combined_counts}
        rm {input.counts}
        """


rule bbduk:
    input:
        trimmed_reads_dir + filename_prefix + "{ID}" + primer_trimmed_filename_suffix        
    output:
        filtered_reads = filtered_reads_dir + filename_prefix + "{ID}" + filtered_filename_suffix,
        filtered_counts = filtered_reads_dir + "{ID}-filtered-counts.tsv"
    log:
        filtered_reads_dir + "{ID}-bbduk.log"
    shell:
        """
        bbduk.sh in={input} out1={output.filtered_reads} qtrim=r trimq=10 mlf=0.5 minavgquality=20 minlength=50 > {log} 2>&1
        paste <( printf "{wildcards.ID}" ) <( grep "Input:" {log} | tr -s " " "\t" | cut -f 2 ) <( grep "Result:" {log} | tr -s " " "\t" | cut -f 2 ) > {output.filtered_counts}
        """


rule combine_bbduk_logs_and_summarize:
    input:
        counts = expand(filtered_reads_dir + "{ID}-filtered-counts.tsv", ID = sample_ID_list),
        logs = expand(filtered_reads_dir + "{ID}-bbduk.log", ID = sample_ID_list)
    output:
        combined_log = filtered_reads_dir + "bbduk.log",
        combined_counts = filtered_reads_dir + "filtered-read-counts.tsv"
    shell:
        """
        cat {input.logs} > {output.combined_log}
        rm {input.logs}

        cat <( printf "sample\tinput_reads\tfiltered_reads\n" ) <( cat {input.counts} ) > {output.combined_counts}
        rm {input.counts}
        """


rule vsearch_derep_sample:
    input:
        filtered_reads_dir + filename_prefix + "{ID}" + filtered_filename_suffix
    output:
        filtered_reads_dir + filename_prefix + "{ID}-derep.fa.tmp"
    shell:
        """
        vsearch --derep_fulllength {input} --strand both --output {output} --sizeout --relabel "sample={wildcards.ID};seq_" > /dev/null 2>&1
        """


rule vsearch_combine_derepd_samples:
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}-derep.fa.tmp", ID = sample_ID_list)
    output:
        filtered_reads_dir + "all-samples.fa.tmp"
    shell:
        """
        cat {input} > {output}
        rm {input}
        """


rule vsearch_process_all:
    input:
        filtered_reads_dir + "all-samples.fa.tmp"
    params:
        all_derep = filtered_reads_dir + "all-samples_derep.fa.tmp",
        rep_seqs = filtered_reads_dir + "rep-seqs.fa.tmp",
        rep_seqs_no_singletons = filtered_reads_dir + "rep-seqs-no-singletons.fa.tmp",
        tmp_counts = filtered_reads_dir + "counts.tmp"
    log:
        "vsearch.log"
    output:
        otus = final_outputs_dir + "OTUs.fasta",
        counts = final_outputs_dir + "counts.tsv"
    shell:
        """
        # dereplicate all
        vsearch --derep_fulllength {input} --strand both --output {params.all_derep} --sizein --sizeout > {log} 2>&1

        # clustering to get rep seqs
        vsearch --cluster_size {params.all_derep} --id 0.97 --strand both --sizein --sizeout --relabel "OTU_" --centroids {params.rep_seqs} > {log} 2>&1

        # removing singletons
        vsearch --sortbysize {params.rep_seqs} --minsize 2 --output {params.rep_seqs_no_singletons} > {log} 2>&1

        # chimera check and removal
        vsearch --uchime_denovo {params.rep_seqs_no_singletons} --sizein --nonchimeras {output.otus} --relabel "OTU_" > {log} 2>&1

        # mapping seqs to OTUs to get OTU abundances per sample
        vsearch --usearch_global {input} -db {output.otus} --sizein --id 0.97 --otutabout {params.tmp_counts} > {log} 2>&1
        sed 's/^#OTU ID/OTU_ID/' {params.tmp_counts} > {output.counts}

        # cleaning up tmp files
        rm {input} {params}
        """


rule zip_biom:
    input:
        final_outputs_dir + "taxonomy-and-counts.biom"
    output:
        final_outputs_dir + "taxonomy-and-counts.biom.zip"
    shell:
        """
        zip -q {final_outputs_dir}taxonomy-and-counts.biom.zip {final_outputs_dir}taxonomy-and-counts.biom && rm {final_outputs_dir}taxonomy-and-counts.biom
        """


rule run_R:
    input:
        otus = final_outputs_dir + "OTUs.fasta",
        counts = final_outputs_dir + "counts.tsv"
    output:
        final_outputs_dir + "taxonomy.tsv",
        final_outputs_dir + "taxonomy-and-counts.biom",
        final_outputs_dir + "taxonomy-and-counts.tsv",
        final_outputs_dir + "read-count-tracking.tsv"
    log:
        "R-processing.log"
    shell:
        """
        Rscript full-R-processing.R > {log} 2>&1
        """


rule raw_fastqc:
    input:
        raw_reads_dir + filename_prefix + "{ID}" + raw_filename_suffix
    output:
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule raw_multiqc:
    input:
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}raw_tmp_qc {raw_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_data.zip {fastqc_out_dir}raw_multiqc_data.zip
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_report.html {fastqc_out_dir}raw_multiqc_report.html
        zip -q {fastqc_out_dir}raw_multiqc_report.html.zip {fastqc_out_dir}raw_multiqc_report.html && rm {fastqc_out_dir}raw_multiqc_report.html
          # removing the individual fastqc files
        rm -rf {raw_reads_dir}*fastqc* {fastqc_out_dir}raw_tmp_qc/
        """


rule filtered_fastqc:
    input:
        filtered_reads_dir + filename_prefix + "{ID}" + filtered_filename_suffix
    output:
        filtered_reads_dir + filename_prefix + "{ID}" + filtered_fastqc_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule filtered_multiqc:
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}" + filtered_fastqc_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "filtered_multiqc_report.html.zip",
        fastqc_out_dir + "filtered_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}filtered_tmp_qc {filtered_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}filtered_tmp_qc/multiqc_data.zip {fastqc_out_dir}filtered_multiqc_data.zip
        mv {fastqc_out_dir}filtered_tmp_qc/multiqc_report.html {fastqc_out_dir}filtered_multiqc_report.html
        zip -q {fastqc_out_dir}filtered_multiqc_report.html.zip {fastqc_out_dir}filtered_multiqc_report.html && rm {fastqc_out_dir}filtered_multiqc_report.html
          # removing the individual fastqc files and temp locations
        rm -rf {filtered_reads_dir}*fastqc* {fastqc_out_dir}filtered_tmp_qc/
        """


rule clean_all:
    shell:
        "rm -rf {needed_dirs} .snakemake/ *.log"
