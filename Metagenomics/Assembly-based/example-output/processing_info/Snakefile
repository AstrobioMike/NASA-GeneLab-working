############################################################################################
## Snakefile for MG assembly-based workflow                                               ##
## Two samples taken from: https://genelab-data.ndc.nasa.gov/genelab/accession/GLDS-286/  ##
##                                                                                        ##
## This file as written expects to be executed within the "processing_info/" directory    ##
## with the raw starting fastq files present in the "../Raw_Data/" directory              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os

########################################
############# General Info #############
########################################

"""
Expected to be run in the following conda environment (or with specified programs versions accessible):

# conda create -y -n GL-mg-assembly -c conda-forge -c bioconda -c defaults -c astrobiomike \
#              fastqc=0.11.9 multiqc=1.8 bbmap=38.86 megahit=1.2.9 bit=1.8.10 bowtie2=2.3.5.1 \
#              samtools=1.9 prodigal=2.6.3 kofamscan=1.3.0 blast=2.9.0 cat=5.1.2 snakemake=5.5.4

# conda activate GL-mg-assembly

The KOFamScan reference files can be downloaded as follows:
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz
# tar -xzvf profiles.tar.gz
# gunzip ko_list.gz

As written, the directory holding them should be in this "KO_DIR" shell env. variable:
# KO_DIR=$(pwd)
    # this would set it if ran in the directory they were just downloaded and unpacked in

This variable can be permanently added to the conda environment like so (modified to be the path to the directory, 
but this location will work if the conda environment was named the same way as above):
# echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect (same goes for CAT_DIR next)

The CAT reference files can be downloaded as follows:
# wget tbb.bio.uu.nl/bastiaan/CAT_prepare/CAT_prepare_20200618.tar.gz
# tar -xvzf CAT_prepare_20200618.tar.gz

As written, the directory holding them should be in this "CAT_DIR" shell env. variable
# CAT_DIR=$(pwd)/CAT_prepare_20200618
    # this would set it if ran in the directory they were just downloaded and unpacked in

This variable can be permanentily added to the conda environment like noted above for the KO_DIR variable:
# echo 'export CAT_DIR=/path/to/CAT_prepare_20200618' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect
"""


########################################
######## Setting some variables ########
########################################

  # current GLDS number
curr_GLDS = "GLDS-286"

  # single column file holding unique portion of sample names
sample_IDs_file = "unique-sample-IDs.txt"

  # useful prefixes and suffixes for filename structure
  # filename prefix (what comes before the unique portion of the name that is provided in the "unique-sample-IDs.txt" file)
filename_prefix = "GLDS-286_metagenomics_"
  # filename suffix (what comes after the unique portion of the name, one for R1 and one for R2)
filename_R1_suffix = "_L003_R1_001_HRremoved.fastq.gz"
filename_R2_suffix = "_L003_R2_001_HRremoved.fastq.gz"
trimmed_filename_R1_suffix = "_L003_R1_001_HRremoved-trimmed.fastq.gz"
trimmed_filename_R2_suffix = "_L003_R2_001_HRremoved-trimmed.fastq.gz"
raw_fastqc_R1_suffix = "_L003_R1_001_HRremoved_fastqc.zip"
raw_fastqc_R2_suffix = "_L003_R2_001_HRremoved_fastqc.zip"
trimmed_fastqc_R1_suffix = "_L003_R1_001_HRremoved-trimmed_fastqc.zip"
trimmed_fastqc_R2_suffix = "_L003_R2_001_HRremoved-trimmed_fastqc.zip"

  # directories (all relative to processing directory)
raw_reads_dir = "../Raw_Data/"
fastqc_out_dir = "../FastQC_Outputs/"
filtered_reads_dir = "../Filtered_Reads/"
assemblies_dir = "../Assemblies/"
genes_dir = "../Predicted_Genes/"
annotations_and_tax_dir = "../Annotations_and_Taxonomy/"
mapping_dir = "../Mapping_Files/"
combined_output_dir = "../Combined_Outputs/"

  # number of threads to use PER snakemake job started (that's determined by the -j parameter passed to the snakemake call)
    # passed to fastqc, bowtie2, and samtools
num_threads = 2

  # number of cpus to use PER snakemake job started
    # passed to KOFamScan and CAT
num_cpus = 2

  # getting KOFamScan ref directory location (see General Info above)
KO_DIR = os.environ.get("KO_DIR")

  # getting CAT ref directory location (see General Info above)
CAT_DIR = os.environ.get("CAT_DIR")


########################################
#### Reading samples file into list ####
########################################

sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in [fastqc_out_dir, filtered_reads_dir, assemblies_dir, genes_dir, 
            annotations_and_tax_dir, mapping_dir, combined_output_dir]:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv",
        expand(annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv", ID = sample_ID_list),
        expand(annotations_and_tax_dir + "{ID}-annotations.tsv", ID = sample_ID_list),
        assemblies_dir + "assembly-summaries.tsv",
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip",
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip"


rule make_combined_tables:
    input:
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list)
    params:
        out_prefix = combined_output_dir + curr_GLDS
    output:
        combined_annots = combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_tax = combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv"
    shell:
        """
        bit-GL-combine-KO-and-tax-tables {input} -o {params.out_prefix}
          # shortening column names to unique part of sample names
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_annots} && rm {output.combined_annots}.tmp
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_tax} && rm {output.combined_tax}.tmp
        """


rule combine_contig_tax_and_coverage:
    input:
        cov = mapping_dir + "{ID}-contig-coverages.tsv",
        tax = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        contig_tmp = annotations_and_tax_dir + "{ID}-contig.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.contig_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}
        cat {params.header_tmp} {params.contig_tmp} > {output}
        rm {params}
        """


rule combine_gene_annots_tax_and_coverage:
    input:
        cov = mapping_dir + "{ID}-gene-coverages.tsv",
        annots = annotations_and_tax_dir + "{ID}-annotations.tsv",
        tax = annotations_and_tax_dir + "{ID}-gene-tax.tsv"
    params:
        gene_tmp = annotations_and_tax_dir + "{ID}-gene.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.annots} | sort -V -k 1 | cut -f 2- ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.gene_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.annots} | cut -f 2- ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}

        cat {params.header_tmp} {params.gene_tmp} > {output}

        rm {params}
        """


rule get_cov_and_det:
    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_dir + "{ID}-genes.fasta"
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        contig_cov_and_det_tmp = mapping_dir + "{ID}-contig-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp",
        contig_cov_tmp = mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv",
        contig_covs = mapping_dir + "{ID}-contig-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.contig_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # contigs
        grep -v "#" {params.contig_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.contig_cov_tmp}
        cat <( printf "contig_ID\tcoverage\n" ) {params.contig_cov_tmp} > {output.contig_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    params:
        index = mapping_dir + "{ID}-index"
    output:
        mapping_dir + "{ID}.bam"
    log:
        mapping_dir + "{ID}-mapping.log"
    shell:
        """
        bowtie2-build {input.assembly} {params.index} > /dev/null 2>&1
        bowtie2 --threads {num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} 2> {log} | samtools view -b | samtools sort -@ {num_threads} > {output} 2> /dev/null
        samtools index -@ {num_threads} {output}

        rm {params.index}*
        """


rule run_tax_classification:
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        AA = genes_dir + "{ID}-genes.faa"
    output:
        gene_tax_out = annotations_and_tax_dir + "{ID}-gene-tax.tsv",
        contig_tax_out = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        CAT_DB = CAT_DIR + "/2020-06-18_database/",
        CAT_TAX = CAT_DIR + "/2020-06-18_taxonomy/",
        tmp_out_prefix = annotations_and_tax_dir + "{ID}-tax-out.tmp",
        tmp_genes = annotations_and_tax_dir + "{ID}-gene-tax.tmp",
        tmp_contigs = annotations_and_tax_dir + "{ID}-contig-tax.tmp"
    shell:
        """
        CAT contigs -d {params.CAT_DB} -t {params.CAT_TAX} -n {num_cpus} -r 3 --top 4 --I_know_what_Im_doing -c {input.assembly} -p {input.AA} -o {params.tmp_out_prefix} > /dev/null 2>&1

        # adding names to gene classifications
        CAT add_names -i {params.tmp_out_prefix}.ORF2LCA.txt -o {params.tmp_genes} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting gene classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "lineage" ) {{ print $1,$2,$4,$5,$6,$7,$8,$9,$10 }} \
        else if ( $2 == "ORF has no hit to database" || $2 ~ /^no taxid found/ ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($2,lineage,";"); print $1,lineage[n],$4,$5,$6,$7,$8,$9,$10 }} }} ' {params.tmp_genes} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/^# ORF/gene_ID/' | sed 's/lineage/taxid/' | \
        sed 's/\*//g' > {output.gene_tax_out}

        # adding names to contig classifications
        CAT add_names -i {params.tmp_out_prefix}.contig2classification.txt -o {params.tmp_contigs} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting contig classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "classification" ) {{ print $1,$4,$6,$7,$8,$9,$10,$11,$12 }} \
        else if ( $2 == "unclassified" ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($4,lineage,";"); print $1,lineage[n],$6,$7,$8,$9,$10,$11,$12 }} }} ' {params.tmp_contigs} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/: [0-9\.]*//g' | sed 's/^# contig/contig_ID/' | \
        sed 's/lineage/taxid/' | sed 's/\*//g' > {output.contig_tax_out}

        rm -rf annotations_and_tax_dir{wildcards.ID}*tmp*
        """


rule run_KO_annotation:
    input:
        genes_dir + "{ID}-genes.faa"
    output:
        annotations_and_tax_dir + "{ID}-annotations.tsv"
    params:
        KO_profiles = KO_DIR + "/profiles/",
        KO_list = KO_DIR + "/ko_list",
        tmp_out = annotations_and_tax_dir + "{ID}-KO-tab.tmp",
        tmp_dir = annotations_and_tax_dir + "{ID}-tmp-KO-dir"
    shell:
        """
        exec_annotation -p {params.KO_profiles} -k {params.KO_list} --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 

        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}

        rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule call_genes:
    input:
        assemblies_dir + "{ID}-assembly.fasta"
    output:
        AA = genes_dir + "{ID}-genes.faa",
        nt = genes_dir + "{ID}-genes.fasta",
        gff = genes_dir + "{ID}-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule summarize_assemblies:
    input:
        expand(assemblies_dir + "{ID}-assembly.fasta", ID = sample_ID_list),
    output:
        assemblies_dir + "assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly -o {output} {input}
        """


rule assemble:
    input:
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        assemblies_dir + "{ID}-assembly.fasta"
    params:
        num_threads
    log:
        assemblies_dir + "{ID}-assembly.log"
    shell:
        """
        megahit -1 {input.R1} -2 {input.R1} -m 500 -t {params} -o {assemblies_dir}{wildcards.ID}-megahit-out > {log} 2>&1
        bit-rename-fasta-headers -i {assemblies_dir}{wildcards.ID}-megahit-out/final.contigs.fa -w c_{wildcards.ID} -o {output}

        rm -rf {assemblies_dir}{wildcards.ID}-megahit-out/
        """


rule raw_fastqc:
    input:
        raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix        
    output:
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix
    shell:
        """
	fastqc {input} -t {num_threads} -q
	"""


rule raw_multiqc:
    input:
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix, ID = sample_ID_list),
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}raw_tmp_qc {raw_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_data.zip {fastqc_out_dir}raw_multiqc_data.zip
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_report.html {fastqc_out_dir}raw_multiqc_report.html
        zip -q {fastqc_out_dir}raw_multiqc_report.html.zip {fastqc_out_dir}raw_multiqc_report.html && rm {fastqc_out_dir}raw_multiqc_report.html
          # removing the individual fastqc files
        rm -rf {raw_reads_dir}*fastqc* {fastqc_out_dir}raw_tmp_qc/
        """


rule bbduk:
    input:
        in1 = raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        in2 = raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix
    output:
        out1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        out2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    log:
        filtered_reads_dir + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l k=17 ftm=5 qtrim=rl \
                trimq=10 mlf=0.5 maxns=0 > {log} 2>&1
        """

rule trimmed_fastqc:
    input:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix
    shell:
        """
	fastqc {input} -t {num_threads} -q
	"""


rule trimmed_multiqc:
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix, ID = sample_ID_list),
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}trimmed_tmp_qc {filtered_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_data.zip {fastqc_out_dir}trimmed_multiqc_data.zip
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_report.html {fastqc_out_dir}trimmed_multiqc_report.html
        zip -q {fastqc_out_dir}trimmed_multiqc_report.html.zip {fastqc_out_dir}trimmed_multiqc_report.html && rm {fastqc_out_dir}trimmed_multiqc_report.html
          # removing the individual fastqc files and temp locations
        rm -rf {filtered_reads_dir}*fastqc* {fastqc_out_dir}trimmed_tmp_qc/
        """


rule clean_all:
    shell:
        "rm -rf {fastqc_out_dir} {filtered_reads_dir} {assemblies_dir} {genes_dir} {annotations_and_tax_dir} {mapping_dir} {combined_output_dir}"


rule clean:
    shell:
        "rm -rf {fastqc_out_dir} {genes_dir} {mapping_dir} {combined_output_dir} {assemblies_dir}assembly-summaries.tsv"
