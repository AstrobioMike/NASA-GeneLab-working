############################################################################################
## Snakefile for GeneLab Illumina metagenomics processing workflow                        ##
## Two samples taken from: https://genelab-data.ndc.nasa.gov/genelab/accession/GLDS-286/  ##
##                                                                                        ##
## This file as written expects to be executed within the "processing_info/" directory    ##
## with the raw starting fastq files present in the "../Raw_Data/" directory              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os

########################################
############# General Info #############
########################################

"""
Expected to be run in the conda environment described in the environment.yml file, or installed as below, or with the specified programs and versions accessible:
This display with all together is for convenience if wanted. If not, or if the environment won't build with all of them together like this, it can also be run with each rule having its own conda environments as shown next:

# conda create -y -n GL-Illumina-metagenomics -c conda-forge -c bioconda -c defaults -c biobakery -c astrobiomike \
#              fastqc=0.11.9 multiqc=1.8 bbmap=38.86 megahit=1.2.9 bit=1.8.11 bowtie2=2.3.5.1 \
#              samtools=1.9 prodigal=2.6.3 kofamscan=1.3.0 blast=2.9.0 cat=5.1.2 metaphlan=3.0.4 \
#              humann=3.0.0.alpha.3 snakemake=5.19.3

# conda activate GL-Illumina-metagenomics

OR, things can be run with `--use-conda` added to the snakemake call, based on the yaml files in the "envs/" subdirectory, though that still requires setting up the databases for each environment as discussed next,
and each variable for a specific program would need to be added to that specific conda environment (if running all in the same conda environment, all would be initially set in that same conda environment)
If doing that, still need to install snakemake in the base conda environment first: 
# conda install -c conda-forge -c bioconda -c defaults snakemake=5.19.3

  ## Additional notes on conda individual rule conda environments ##
    # By default snakemake will make the new conda environments every time it runs in a new location, but you can tell it to use the same ones by providing `--conda-prefix <path/to/conda/envs>` in addition to `--use-conda`
    # If this is the first time setting up, it can be run with the `--conda-create-envs-only` flag to generate the conda environments, then again with `--list-conda-envs` and the `-n` flag (dry run), this will
      # show where the conda environments are/what their names are, so we can activate them individually in order to set up the following required databases (only needed on initial setup)
    # example run using individual conda envs: snakemake -j 4 --use-conda --conda-prefix <path/to/conda/envs> -p

### Reference databases that require one-time setup ###

The KOFamScan reference files can be downloaded as follows:
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz
# tar -xzvf profiles.tar.gz
# gunzip ko_list.gz

As written, the directory holding them should be in this "KO_DIR" shell env. variable:
This variable can be permanently added to the conda environment like so (modified to be the path to the actual directory):
# mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d/
# echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect

The CAT reference files can be downloaded as follows:
# wget tbb.bio.uu.nl/bastiaan/CAT_prepare/CAT_prepare_20200618.tar.gz
# tar -xvzf CAT_prepare_20200618.tar.gz

As written, the directory holding them should be in the "CAT_DIR" shell env. variable
This variable can be permanentily added to the conda environment like noted above for the KO_DIR variable:
# mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d/
# echo 'export CAT_DIR=/path/to/CAT_prepare_20200618' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect

# humann3 and metaphlan3 databases downloaded with the following:
# humann3_databases --download chocophlan full <location>
# humann3_databases --download uniref uniref90_ec_filtered_diamond <location>
# humann3_databases --download utility_mapping full <location>
# metaphlan --install --bowtie2db <location>

The humann3 db locations are automatically tracked by the program, if not putting metaphlan in the default location, 
it can be added to the conda environment same as above:
# mkdir -p ${CONDA_PREFIX}/etc/conda/activate.d/
# echo 'export MPA_DIR=/path/to/metaphlan3-db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect
"""


########################################
######## Setting some variables ########
########################################

  # current GLDS number
curr_GLDS = "GLDS-286"

  # single column file holding unique portion of sample names
sample_IDs_file = "unique-sample-IDs.txt"

  # useful prefixes and suffixes for filename structure
  # filename prefix (what comes before the unique portion of the name that is provided in the "unique-sample-IDs.txt" file)
filename_prefix = "GLDS-286_metagenomics_"
  # filename suffix (what comes after the unique portion of the name, one for R1 and one for R2)
filename_R1_suffix = "_L003_R1_001_HRremoved.fastq.gz"
filename_R2_suffix = "_L003_R2_001_HRremoved.fastq.gz"
trimmed_filename_R1_suffix = "_L003_R1_001_HRremoved-trimmed.fastq.gz"
trimmed_filename_R2_suffix = "_L003_R2_001_HRremoved-trimmed.fastq.gz"
raw_fastqc_R1_suffix = "_L003_R1_001_HRremoved_fastqc.zip"
raw_fastqc_R2_suffix = "_L003_R2_001_HRremoved_fastqc.zip"
trimmed_fastqc_R1_suffix = "_L003_R1_001_HRremoved-trimmed_fastqc.zip"
trimmed_fastqc_R2_suffix = "_L003_R2_001_HRremoved-trimmed_fastqc.zip"

  # directories (all relative to processing directory)
raw_reads_dir = "../Raw_Data/"
fastqc_out_dir = "../FastQC_Outputs/"
filtered_reads_dir = "../Filtered_Sequence_Data/"
assembly_based_dir = "../Assembly-based_Processing/"
assemblies_dir = assembly_based_dir + "Assemblies/"
genes_dir = assembly_based_dir + "Predicted_Genes/"
annotations_and_tax_dir = assembly_based_dir + "Annotations_and_Taxonomy/"
mapping_dir = assembly_based_dir + "Mapping_Files/"
combined_output_dir = assembly_based_dir + "Combined_Outputs/"
read_based_dir = "../Read-based_Processing/"

dirs_to_create = [fastqc_out_dir, filtered_reads_dir, assembly_based_dir, assemblies_dir,
                  genes_dir, annotations_and_tax_dir, mapping_dir, combined_output_dir,
                  read_based_dir]

  # number of threads to use PER snakemake job started (which is determined by the -j parameter passed to the snakemake call)
    # passed to fastqc, bowtie2, samtools, humann3
num_threads = 7

  # number of cpus to use PER snakemake job started
    # passed to KOFamScan and CAT
num_cpus = 8

  # max memory allowed passed to megahit assembler
    # either by proportion of available on system, e.g. 0.5
    # or by absolute value in bytes, e.g. 100e9 would be 100 GB
max_mem = 0.5


########################################
#### Reading samples file into list ####
########################################

sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in dirs_to_create:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv",
        expand(annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv", ID = sample_ID_list),
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list),
        assemblies_dir + "assembly-summaries.tsv",
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip",
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip",
        read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv",
        read_based_dir + curr_GLDS + "-metaphlan-taxonomy.tsv"
    shell:
        """
        # copying log file to store with processing info
        cp .snakemake/log/$(ls -t .snakemake/log/ | head -n 1) snakemake-run.log
        """


rule run_humann3:
    """
    This rule runs humann3 and metaphlan3 on each individual sample generating the
    read-based functional annotations and taxonomic classifications.
    """
    conda:
        "envs/humann3.yaml"
    input:
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    params:
        combined_reads = read_based_dir + "{ID}-reads.tmp.fq.gz",
        output_dir = read_based_dir + "{ID}-humann3-out-dir",
        tmp_metaphlan = read_based_dir + "{ID}-humann3-out-dir/{ID}_humann_temp/{ID}_metaphlan_bugs_list.tsv",
        tmp_dir = read_based_dir + "{ID}-humann3-out-dir/{ID}_humann_temp/"
    output:
        read_based_dir + "{ID}-humann3-out-dir/{ID}_genefamilies.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_pathabundance.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_pathcoverage.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_metaphlan_bugs_list.tsv"
    shell:
        """
        cat {input.R1} {input.R2} > {params.combined_reads}
        humann --input {params.combined_reads} --output {params.output_dir} --threads {num_threads} --output-basename {wildcards.ID} --metaphlan-options "--bowtie2db ${{MPA_DIR}} --unknown_estimation --add_viruses --sample_id {wildcards.ID}" --bowtie-options "--sensitive --mm" > /dev/null 2>&1
        mv {params.tmp_metaphlan} {output[3]}
        rm -rf {params.combined_reads} {params.tmp_dir}
        """


rule combine_read_based_processing_tables:
    """
    This rule combines the read-based humann3 output functional tables from indiviual samples into single 
    tables across the GLDS dataset.
    """
    conda:
        "envs/humann3.yaml"
    input:
        gene_families = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_genefamilies.tsv", ID = sample_ID_list),
        path_abundances = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_pathabundance.tsv", ID = sample_ID_list),
        path_coverages = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_pathcoverage.tsv", ID = sample_ID_list)
    params:
        gene_fam_dir = read_based_dir + "gene-family-results/",
        path_abund_dir = read_based_dir + "path-abundance-results/",
        path_cov_dir = read_based_dir + "path-coverage-results/",
    output:
        gene_families = read_based_dir + "gene-families-initial.tsv",
        path_abundances = read_based_dir + "pathway-abundances-initial.tsv",
        path_coverages = read_based_dir + "pathway-coverages-initial.tsv"
    shell:
        """
          # they all need to be in the same directory to be merged
        mkdir -p {params}
        cp {input.gene_families} {params.gene_fam_dir}
        cp {input.path_abundances} {params.path_abund_dir}
        cp {input.path_coverages} {params.path_cov_dir}

        humann_join_tables -i {params.gene_fam_dir} -o {output.gene_families} > /dev/null 2>&1
        humann_join_tables -i {params.path_abund_dir} -o {output.path_abundances} > /dev/null 2>&1
        humann_join_tables -i {params.path_cov_dir} -o {output.path_coverages} > /dev/null 2>&1

        rm -rf {params}
        """


rule split_read_based_processing_tables:
    """
    The read-based functional annotation tables have taxonomic info and non-taxonomic info mixed
    together initially. humann comes with utility scripts to split these. This rule does that,
    generating non-taxonomically grouped functional info files and taxonomically grouped ones.
    """
    conda:
        "envs/humann3.yaml"
    input:
        gene_families = read_based_dir + "gene-families-initial.tsv",
        path_abundances = read_based_dir + "pathway-abundances-initial.tsv",
        path_coverages = read_based_dir + "pathway-coverages-initial.tsv"
    output:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv",
        gene_families_grouped = read_based_dir + curr_GLDS + "-gene-families-grouped-by-taxa.tsv",
        path_abundances = read_based_dir + curr_GLDS + "-pathway-abundances.tsv",
        path_abundances_grouped = read_based_dir + curr_GLDS + "-pathway-abundances-grouped-by-taxa.tsv",
        path_coverages = read_based_dir + curr_GLDS + "-pathway-coverages.tsv",
        path_coverages_grouped = read_based_dir + curr_GLDS + "-pathway-coverages-grouped-by-taxa.tsv",
    shell:
        """
        humann_split_stratified_table -i {input.gene_families} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}gene-families-initial_stratified.tsv {output.gene_families_grouped}
        mv {read_based_dir}gene-families-initial_unstratified.tsv {output.gene_families}

        humann_split_stratified_table -i {input.path_abundances} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}pathway-abundances-initial_stratified.tsv {output.path_abundances_grouped}
        mv {read_based_dir}pathway-abundances-initial_unstratified.tsv {output.path_abundances}

        humann_split_stratified_table -i {input.path_coverages} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}pathway-coverages-initial_stratified.tsv {output.path_coverages_grouped}
        mv {read_based_dir}pathway-coverages-initial_unstratified.tsv {output.path_coverages}

        rm {input}
        """


rule gen_normalized_read_based_processing_tables:
    """
    This rule generates some normalized tables of the read-based functional outputs from
    humann that are more readily suitable for across sample comparisons.
    """
    conda:
        "envs/humann3.yaml"
    input:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv",
        path_abundances = read_based_dir + curr_GLDS + "-pathway-abundances.tsv"
    output:
        gene_families_cpm = read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        path_abundances_cpm = read_based_dir + curr_GLDS + "-pathway-abundances-cpm.tsv"
    shell:
        """
        humann_renorm_table -i {input.gene_families} -o {output.gene_families_cpm} --update-snames > /dev/null 2>&1
        humann_renorm_table -i {input.path_abundances} -o {output.path_abundances_cpm} --update-snames > /dev/null 2>&1
        """


rule gen_read_based_processing_KO_table:
    """
    This rule summarizes the read-based humann annotations based on Kegg Orthlogy terms.
    """
    conda:
        "envs/humann3.yaml"
    input:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv"
    output:
        gene_families_KOs_cpm = read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv"
    shell:
        """
        humann_regroup_table -i {input} -g uniref90_ko 2> /dev/null | humann_rename_table -n kegg-orthology 2> /dev/null | humann_renorm_table -o {output} --update-snames > /dev/null 2>&1
        """


rule combine_read_based_processing_taxonomy:
    """
    This rule includes final outputs from read-based functional annotation process as inputs even though they aren't used just so
    we can delete those working directories when done with them here (ensuring the other processes are already done with them).
    """
    conda:
        "envs/humann3.yaml"
    input:
        in_files = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_metaphlan_bugs_list.tsv", ID = sample_ID_list),
        trigger1 = read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        trigger2 = read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv"
    output:
        read_based_dir + curr_GLDS + "-metaphlan-taxonomy.tsv"
    shell:
        """
        merge_metaphlan_tables.py {input.in_files} > {output} 2> /dev/null

        # removing redundant text from headers (using the -i flag to keep it portable with darwin shell)
        sed -i.tmp 's/_metaphlan_bugs_list//g' {output}
        rm {output}.tmp
        """


rule make_combined_tables:
    conda:
        "envs/bit.yaml"
    input:
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list)
    params:
        out_prefix = combined_output_dir + curr_GLDS
    output:
        combined_annots = combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_tax = combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv"
    shell:
        """
        bit-GL-combine-KO-and-tax-tables {input} -o {params.out_prefix}
          # shortening column names to unique part of sample names
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_annots} && rm {output.combined_annots}.tmp
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_tax} && rm {output.combined_tax}.tmp
        """


rule combine_contig_tax_and_coverage:
    """
    This rule combines the contig-level taxonomic and coverage information for each individual sample. 
    """
    input:
        cov = mapping_dir + "{ID}-contig-coverages.tsv",
        tax = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        contig_tmp = annotations_and_tax_dir + "{ID}-contig.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-contig-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.contig_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}
        cat {params.header_tmp} {params.contig_tmp} > {output}
        rm {params} {input}
        """


rule combine_gene_annots_tax_and_coverage:
    """
    This rule combines the gene-level functional annotations, taxonomic classifications, and coverage information for each individual sample.
    """
    input:
        cov = mapping_dir + "{ID}-gene-coverages.tsv",
        annots = annotations_and_tax_dir + "{ID}-annotations.tsv",
        tax = annotations_and_tax_dir + "{ID}-gene-tax.tsv"
    params:
        gene_tmp = annotations_and_tax_dir + "{ID}-gene.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-gene-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.annots} | sort -V -k 1 | cut -f 2- ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.gene_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.annots} | cut -f 2- ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}

        cat {params.header_tmp} {params.gene_tmp} > {output}

        rm {params} {input}
        """


rule get_cov_and_det:
    """
    This rule pulls out coverage and detection information for each sample, gene-level and contig-level,
    and filters the coverage information based on requiring at least 50% detection.
    """
    conda:
        "envs/mapping.yaml"
    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_dir + "{ID}-genes.fasta"
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        contig_cov_and_det_tmp = mapping_dir + "{ID}-contig-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp",
        contig_cov_tmp = mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv",
        contig_covs = mapping_dir + "{ID}-contig-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.contig_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # contigs
        grep -v "#" {params.contig_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.contig_cov_tmp}
        cat <( printf "contig_ID\tcoverage\n" ) {params.contig_cov_tmp} > {output.contig_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    """
    This rule builds the bowtie2 index and runs the mapping for each sample.
    """
    conda:
        "envs/mapping.yaml"
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    params:
        index = mapping_dir + "{ID}-index"
    output:
        mapping_dir + "{ID}.bam"
    log:
        mapping_dir + "{ID}-mapping.log"
    shell:
        """
        bowtie2-build {input.assembly} {params.index} > /dev/null 2>&1
        bowtie2 --threads {num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} 2> {log} | samtools view -b | samtools sort -@ {num_threads} > {output} 2> /dev/null
        samtools index -@ {num_threads} {output}

        rm {params.index}*
        """


rule run_tax_classification:
    """
    This rule runs the gene- and contig-level taxonomic classifications for each sample.
    """
    conda:
        "envs/cat.yaml"
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        AA = genes_dir + "{ID}-genes.faa"
    output:
        gene_tax_out = annotations_and_tax_dir + "{ID}-gene-tax.tsv",
        contig_tax_out = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        tmp_out_prefix = annotations_and_tax_dir + "{ID}-tax-out.tmp",
        tmp_genes = annotations_and_tax_dir + "{ID}-gene-tax.tmp",
        tmp_contigs = annotations_and_tax_dir + "{ID}-contig-tax.tmp"
    shell:
        """
        CAT contigs -d ${{CAT_DIR}}/2020-06-18_database/ -t ${{CAT_DIR}}/2020-06-18_taxonomy/ -n {num_cpus} -r 3 --top 4 --I_know_what_Im_doing -c {input.assembly} -p {input.AA} -o {params.tmp_out_prefix} > /dev/null 2>&1

        # adding names to gene classifications
        CAT add_names -i {params.tmp_out_prefix}.ORF2LCA.txt -o {params.tmp_genes} -t ${{CAT_DIR}}/2020-06-18_taxonomy/ --only_official > /dev/null 2>&1

        # formatting gene classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "lineage" ) {{ print $1,$2,$4,$5,$6,$7,$8,$9,$10 }} \
        else if ( $2 == "ORF has no hit to database" || $2 ~ /^no taxid found/ ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($2,lineage,";"); print $1,lineage[n],$4,$5,$6,$7,$8,$9,$10 }} }} ' {params.tmp_genes} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/^# ORF/gene_ID/' | sed 's/lineage/taxid/' | \
        sed 's/\*//g' > {output.gene_tax_out}

        # adding names to contig classifications
        CAT add_names -i {params.tmp_out_prefix}.contig2classification.txt -o {params.tmp_contigs} -t ${{CAT_DIR}}/2020-06-18_taxonomy/ --only_official > /dev/null 2>&1

        # formatting contig classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "classification" ) {{ print $1,$4,$6,$7,$8,$9,$10,$11,$12 }} \
        else if ( $2 == "unclassified" ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($4,lineage,";"); print $1,lineage[n],$6,$7,$8,$9,$10,$11,$12 }} }} ' {params.tmp_contigs} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/: [0-9\.]*//g' | sed 's/^# contig/contig_ID/' | \
        sed 's/lineage/taxid/' | sed 's/\*//g' > {output.contig_tax_out}

        rm -rf {annotations_and_tax_dir}{wildcards.ID}*tmp*
        """


rule run_KO_annotation:
    """
    This rule runs the gene-level (KO) functional annotation for each sample.
    """
    conda:
        "envs/kofamscan.yaml"
    input:
        genes_dir + "{ID}-genes.faa"
    output:
        annotations_and_tax_dir + "{ID}-annotations.tsv"
    params:
        tmp_out = annotations_and_tax_dir + "{ID}-KO-tab.tmp",
        tmp_dir = annotations_and_tax_dir + "{ID}-tmp-KO-dir"
    shell:
        """
        exec_annotation -p ${{KO_DIR}}/profiles/ -k ${{KO_DIR}}/ko_list --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 

        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}

        rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule call_genes:
    """
    This rule calls genes on each sample's assembly file.
    """
    conda:
        "envs/prodigal.yaml"
    input:
        assemblies_dir + "{ID}-assembly.fasta"
    output:
        AA = genes_dir + "{ID}-genes.faa",
        nt = genes_dir + "{ID}-genes.fasta",
        gff = genes_dir + "{ID}-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule summarize_assemblies:
    """
    This rule summarizes and reports general stats for all individual sample assemblies in one table.
    """
    conda:
        "envs/bit.yaml"
    input:
        expand(assemblies_dir + "{ID}-assembly.fasta", ID = sample_ID_list),
    output:
        assemblies_dir + "assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly -o {output} {input}
        """


rule assemble:
    """
    This rule handles running the assembly for each individual sample.
    """
    conda:
        "envs/megahit.yaml"
    input:
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        assemblies_dir + "{ID}-assembly.fasta"
    log:
        assemblies_dir + "{ID}-assembly.log"
    shell:
        """
        # removing output directory if exists already but rule still needs to be run (because there is no --force option to megahit i dont't think):
        rm -rf {assemblies_dir}{wildcards.ID}-megahit-out/

        megahit -1 {input.R1} -2 {input.R1} -m {max_mem} -t {num_threads} --min-contig-len 500 -o {assemblies_dir}{wildcards.ID}-megahit-out > {log} 2>&1
        bit-rename-fasta-headers -i {assemblies_dir}{wildcards.ID}-megahit-out/final.contigs.fa -w c_{wildcards.ID} -o {output}

        rm -rf {assemblies_dir}{wildcards.ID}-megahit-out/
        """


rule raw_fastqc:
    """
    This rule runs fastqc on all raw input fastq files.
    """
    conda:
        "envs/qc.yaml"
    input:
        raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix        
    output:
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix
    shell:
        """
        fastqc {input} -t {num_threads} -q
        """


rule raw_multiqc:
    """
    This rule collates all raw fastqc outputs.
    """
    conda:
        "envs/qc.yaml"
    input:
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix, ID = sample_ID_list),
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}raw_tmp_qc {raw_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_data.zip {fastqc_out_dir}raw_multiqc_data.zip
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_report.html {fastqc_out_dir}raw_multiqc_report.html
        zip -q {fastqc_out_dir}raw_multiqc_report.html.zip {fastqc_out_dir}raw_multiqc_report.html && rm {fastqc_out_dir}raw_multiqc_report.html
          # removing the individual fastqc files
        rm -rf {raw_reads_dir}*fastqc* {fastqc_out_dir}raw_tmp_qc/
        """


rule bbduk:
    """
    This rule runs quality filtering/trimming on raw input fastq files for each individual sample.
    """
    conda:
        "envs/qc.yaml"
    input:
        in1 = raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        in2 = raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix
    output:
        out1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        out2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    log:
        filtered_reads_dir + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l k=17 ftm=5 qtrim=rl \
                trimq=10 mlf=0.5 maxns=0 > {log} 2>&1
        """


rule trimmed_fastqc:
    """
    This rule runs fastqc on all trimmed/filtered input fastq files.
    """
    conda:
        "envs/qc.yaml"
    input:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix
    shell:
        """
        fastqc {input} -t {num_threads} -q
        """


rule trimmed_multiqc:
    """
    This rule collates all trimmed/filtered fastqc outputs.
    """
    conda:
        "envs/qc.yaml"
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix, ID = sample_ID_list),
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}trimmed_tmp_qc {filtered_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_data.zip {fastqc_out_dir}trimmed_multiqc_data.zip
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_report.html {fastqc_out_dir}trimmed_multiqc_report.html
        zip -q {fastqc_out_dir}trimmed_multiqc_report.html.zip {fastqc_out_dir}trimmed_multiqc_report.html && rm {fastqc_out_dir}trimmed_multiqc_report.html
          # removing the individual fastqc files and temp locations
        rm -rf {filtered_reads_dir}*fastqc* {fastqc_out_dir}trimmed_tmp_qc/
        """


rule clean_all:
    shell:
        "rm -rf {dirs_to_create} .snakemake/ snakemake-run.log"
