############################################################################################
## Snakefile for GeneLab Illumina metagenomics processing workflow                        ##
## Two samples taken from: https://genelab-data.ndc.nasa.gov/genelab/accession/GLDS-286/  ##
##                                                                                        ##
## This file as written expects to be executed within the "processing_info/" directory    ##
## with the raw starting fastq files present in the "../Raw_Data/" directory              ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os

########################################
############# General Info #############
########################################

"""
Expected to be run in the conda environment described in the environment.yml file, or installed as below, or with the specified programs and versions accessible:

# conda create -y -n GL-Illumina-metagenomics -c conda-forge -c bioconda -c defaults -c biobakery -c astrobiomike \
#              fastqc=0.11.9 multiqc=1.8 bbmap=38.86 megahit=1.2.9 bit=1.8.11 bowtie2=2.3.5.1 \
#              samtools=1.9 prodigal=2.6.3 kofamscan=1.3.0 blast=2.9.0 cat=5.1.2 metaphlan=3.0.4 \
#              humann=3.0.0.alpha.3 snakemake=5.5.4

# conda activate GL-Illumina-metagenomics

The KOFamScan reference files can be downloaded as follows:
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz
# tar -xzvf profiles.tar.gz
# gunzip ko_list.gz

As written, the directory holding them should be in this "KO_DIR" shell env. variable:
# KO_DIR=$(pwd)
    # this would set it if ran in the directory they were just downloaded and unpacked in

This variable can be permanently added to the conda environment like so (modified to be the path to the directory, 
but this location will work if the conda environment was named the same way as above):
# echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect (same goes for CAT_DIR next)

The CAT reference files can be downloaded as follows:
# wget tbb.bio.uu.nl/bastiaan/CAT_prepare/CAT_prepare_20200618.tar.gz
# tar -xvzf CAT_prepare_20200618.tar.gz

As written, the directory holding them should be in this "CAT_DIR" shell env. variable
# CAT_DIR=$(pwd)/CAT_prepare_20200618
    # this would set it if ran in the directory they were just downloaded and unpacked in

This variable can be permanentily added to the conda environment like noted above for the KO_DIR variable:
# echo 'export CAT_DIR=/path/to/CAT_prepare_20200618' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment for it to take effect

# humann3 and metaphlan3 databases downloaded with the following:
# humann3_databases --download chocophlan full
# humann3_databases --download uniref uniref90_diamond
# humann3_databases --download utility_mapping full
# metaphlan --install

Again, any not put in the default locations the programs are searching can be added to the conda environment like shown above.
"""

########################################
######## Setting some variables ########
########################################

  # current GLDS number
curr_GLDS = "GLDS-286"

  # single column file holding unique portion of sample names
sample_IDs_file = "unique-sample-IDs.txt"

  # useful prefixes and suffixes for filename structure
  # filename prefix (what comes before the unique portion of the name that is provided in the "unique-sample-IDs.txt" file)
filename_prefix = "GLDS-286_metagenomics_"
  # filename suffix (what comes after the unique portion of the name, one for R1 and one for R2)
filename_R1_suffix = "_L003_R1_001_HRremoved.fastq.gz"
filename_R2_suffix = "_L003_R2_001_HRremoved.fastq.gz"
trimmed_filename_R1_suffix = "_L003_R1_001_HRremoved-trimmed.fastq.gz"
trimmed_filename_R2_suffix = "_L003_R2_001_HRremoved-trimmed.fastq.gz"
raw_fastqc_R1_suffix = "_L003_R1_001_HRremoved_fastqc.zip"
raw_fastqc_R2_suffix = "_L003_R2_001_HRremoved_fastqc.zip"
trimmed_fastqc_R1_suffix = "_L003_R1_001_HRremoved-trimmed_fastqc.zip"
trimmed_fastqc_R2_suffix = "_L003_R2_001_HRremoved-trimmed_fastqc.zip"

  # directories (all relative to processing directory)
raw_reads_dir = "../Raw_Data/"
fastqc_out_dir = "../FastQC_Outputs/"
filtered_reads_dir = "../Filtered_Sequence_Data/"
assembly_based_dir = "../Assembly-based_Processing/"
assemblies_dir = assembly_based_dir + "Assemblies/"
genes_dir = assembly_based_dir + "Predicted_Genes/"
annotations_and_tax_dir = assembly_based_dir + "Annotations_and_Taxonomy/"
mapping_dir = assembly_based_dir + "Mapping_Files/"
combined_output_dir = assembly_based_dir + "Combined_Outputs/"
read_based_dir = "../Read-based_Processing/"

dirs_to_create = [assembly_based_dir, assemblies_dir,
                  genes_dir, annotations_and_tax_dir,
                  mapping_dir, combined_output_dir,
                  read_based_dir]

  # number of threads to use PER snakemake job started (which is determined by the -j parameter passed to the snakemake call)
    # passed to fastqc, bowtie2, samtools, humann3
num_threads = 6

  # number of cpus to use PER snakemake job started
    # passed to KOFamScan and CAT
num_cpus = 6

  # max memory allowed passed to megahit assembler
    # either by proportion of available on system, e.g. 0.5
    # or by absolute value in bytes, e.g. 100e9 would be 100 GB
max_mem = 0.5

  # getting KOFamScan ref directory location (see General Info above)
KO_DIR = os.environ.get("KO_DIR")

  # getting CAT ref directory location
CAT_DIR = os.environ.get("CAT_DIR")

  # getting metaphlan ref directory location
MPA_DIR = os.environ.get("METAPHLAN_DIR")


########################################
#### Reading samples file into list ####
########################################

sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in dirs_to_create:
	try:
		os.mkdir(dir)
	except:
		pass


########################################
############# Rules start ##############
########################################

rule all:
    input:
        combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv",
        expand(annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv", ID = sample_ID_list),
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list),
        assemblies_dir + "assembly-summaries.tsv",
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip",
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip",
        read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv",
        read_based_dir + curr_GLDS + "-metaphlan-taxonomy.tsv"
    shell:
        """
        # copying log file to store with processing info
        cp .snakemake/log/$(ls -t .snakemake/log/ | head -n 1) snakemake-run.log
        """


rule run_humann3:
    """
    This rule runs humann3 and metaphlan3 on each individual sample generating the
    read-based functional annotations and taxonomic classifications.
    """
    input:
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    params:
        combined_reads = read_based_dir + "{ID}-reads.tmp.fq.gz",
        output_dir = read_based_dir + "{ID}-humann3-out-dir"
    output:
        read_based_dir + "{ID}-humann3-out-dir/{ID}_genefamilies.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_pathabundance.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_pathcoverage.tsv",
        read_based_dir + "{ID}-humann3-out-dir/{ID}_humann_temp/{ID}_metaphlan_bugs_list.tsv"
    shell:
        """
        cat {input.R1} {input.R1} > {params.combined_reads}
        humann --input {params.combined_reads} --output {params.output_dir} --threads {num_threads} --output-basename {wildcards.ID} --metaphlan-options "--bowtie2db {MPA_DIR} --unknown_estimation --add_viruses --sample_id {wildcards.ID}" > /dev/null 2>&1
        rm {params.combined_reads}
        """


rule combine_read_based_processing_tables:
    """
    This rule combines the read-based humann3 output functional tables from indiviual samples into single 
    tables across the GLDS dataset.
    """
    input:
        gene_families = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_genefamilies.tsv", ID = sample_ID_list),
        path_abundances = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_pathabundance.tsv", ID = sample_ID_list),
        path_coverages = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_pathcoverage.tsv", ID = sample_ID_list)
    params:
        gene_fam_dir = read_based_dir + "gene-family-results/",
        path_abund_dir = read_based_dir + "path-abundance-results/",
        path_cov_dir = read_based_dir + "path-coverage-results/",
    output:
        gene_families = read_based_dir + "gene-families-initial.tsv",
        path_abundances = read_based_dir + "pathway-abundances-initial.tsv",
        path_coverages = read_based_dir + "pathway-coverages-initial.tsv"
    shell:
        """
          # they all need to be in the same directory to be merged
        mkdir -p {params}
        cp {input.gene_families} {params.gene_fam_dir}
        cp {input.path_abundances} {params.path_abund_dir}
        cp {input.path_coverages} {params.path_cov_dir}

        humann_join_tables -i {params.gene_fam_dir} -o {output.gene_families} > /dev/null 2>&1
        humann_join_tables -i {params.path_abund_dir} -o {output.path_abundances} > /dev/null 2>&1
        humann_join_tables -i {params.path_cov_dir} -o {output.path_coverages} > /dev/null 2>&1

        rm -rf {params}
        """


rule split_read_based_processing_tables:
    """
    The read-based functional annotation tables have taxonomic info and non-taxonomic info mixed
    together initially. humann comes with utility scripts to split these. This rule does that,
    generating non-taxonomically grouped functional info files and taxonomically grouped ones.
    """
    input:
        gene_families = read_based_dir + "gene-families-initial.tsv",
        path_abundances = read_based_dir + "pathway-abundances-initial.tsv",
        path_coverages = read_based_dir + "pathway-coverages-initial.tsv"
    output:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv",
        gene_families_grouped = read_based_dir + curr_GLDS + "-gene-families-grouped-by-taxa.tsv",
        path_abundances = read_based_dir + curr_GLDS + "-pathway-abundances.tsv",
        path_abundances_grouped = read_based_dir + curr_GLDS + "-pathway-abundances-grouped-by-taxa.tsv",
        path_coverages = read_based_dir + curr_GLDS + "-pathway-coverages.tsv",
        path_coverages_grouped = read_based_dir + curr_GLDS + "-pathway-coverages-grouped-by-taxa.tsv",
    shell:
        """
        humann_split_stratified_table -i {input.gene_families} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}gene-families-initial_stratified.tsv {output.gene_families_grouped}
        mv {read_based_dir}gene-families-initial_unstratified.tsv {output.gene_families}

        humann_split_stratified_table -i {input.path_abundances} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}pathway-abundances-initial_stratified.tsv {output.path_abundances_grouped}
        mv {read_based_dir}pathway-abundances-initial_unstratified.tsv {output.path_abundances}

        humann_split_stratified_table -i {input.path_coverages} -o {read_based_dir} > /dev/null 2>&1
        mv {read_based_dir}pathway-coverages-initial_stratified.tsv {output.path_coverages_grouped}
        mv {read_based_dir}pathway-coverages-initial_unstratified.tsv {output.path_coverages}

        rm {input}
        """


rule gen_normalized_read_based_processing_tables:
    """
    This rule generates some normalized tables of the read-based functional outputs from
    humann that are more readily suitable for across sample comparisons.
    """
    input:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv",
        path_abundances = read_based_dir + curr_GLDS + "-pathway-abundances.tsv"
    output:
        gene_families_cpm = read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        path_abundances_cpm = read_based_dir + curr_GLDS + "-pathway-abundances-cpm.tsv"
    shell:
        """
        humann_renorm_table -i {input.gene_families} -o {output.gene_families_cpm} --update-snames > /dev/null 2>&1
        humann_renorm_table -i {input.path_abundances} -o {output.path_abundances_cpm} --update-snames > /dev/null 2>&1
        """


rule gen_read_based_processing_KO_table:
    """
    This rule summarizes the read-based humann annotations based on Kegg Orthlogy terms.
    """
    input:
        gene_families = read_based_dir + curr_GLDS + "-gene-families.tsv"
    output:
        gene_families_KOs_cpm = read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv"
    shell:
        """
        humann_regroup_table -i {input} -g uniref90_ko 2> /dev/null | humann_rename_table -n kegg-orthology 2> /dev/null | humann_renorm_table -o {output} --update-snames > /dev/null 2>&1
        """


rule combine_read_based_processing_taxonomy:
    """
    This rule includes final outputs from read-based functional annotation process as inputs even though they aren't used just so 
    we can delete those working directories when done with them here (ensuring the other processes are already done with them).
    """

    input:
        in_files = expand(read_based_dir + "{ID}-humann3-out-dir/{ID}_humann_temp/{ID}_metaphlan_bugs_list.tsv", ID = sample_ID_list),
        trigger1 = read_based_dir + curr_GLDS + "-gene-families-cpm.tsv",
        trigger2 = read_based_dir + curr_GLDS + "-gene-families-KO-cpm.tsv"
    output:
        read_based_dir + curr_GLDS + "-metaphlan-taxonomy.tsv"
    shell:
        """
        merge_metaphlan_tables.py {input.in_files} > {output} 2> /dev/null

        # removing redundant text from headers (using the -i flag to keep it portable with darwin shell)
        sed -i.tmp 's/_metaphlan_bugs_list//g' {output}
        rm {output}.tmp

        # removing humann3 working directories
        rm -rf {read_based_dir}*-humann3-out-dir/
        """


rule make_combined_tables:
    input:
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list)
    params:
        out_prefix = combined_output_dir + curr_GLDS
    output:
        combined_annots = combined_output_dir + curr_GLDS + "-KO-function-coverages.tsv",
        combined_tax = combined_output_dir + curr_GLDS + "-taxonomy-coverages.tsv"
    shell:
        """
        bit-GL-combine-KO-and-tax-tables {input} -o {params.out_prefix}
          # shortening column names to unique part of sample names
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_annots} && rm {output.combined_annots}.tmp
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_tax} && rm {output.combined_tax}.tmp
        """


rule combine_contig_tax_and_coverage:
    """
    This rule combines the contig-level taxonomic and coverage information for each individual sample. 
    """
    input:
        cov = mapping_dir + "{ID}-contig-coverages.tsv",
        tax = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        contig_tmp = annotations_and_tax_dir + "{ID}-contig.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-contig-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.contig_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}
        cat {params.header_tmp} {params.contig_tmp} > {output}
        rm {params} {input}
        """


rule combine_gene_annots_tax_and_coverage:
    """
    This rule combines the gene-level functional annotations, taxonomic classifications, and coverage information for each individual sample.
    """
    input:
        cov = mapping_dir + "{ID}-gene-coverages.tsv",
        annots = annotations_and_tax_dir + "{ID}-annotations.tsv",
        tax = annotations_and_tax_dir + "{ID}-gene-tax.tsv"
    params:
        gene_tmp = annotations_and_tax_dir + "{ID}-gene.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-gene-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.annots} | sort -V -k 1 | cut -f 2- ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.gene_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.annots} | cut -f 2- ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}

        cat {params.header_tmp} {params.gene_tmp} > {output}

        rm {params} {input}
        """


rule get_cov_and_det:
    """
    This rule pulls out coverage and detection information for each sample, gene-level and contig-level,
    and filters the coverage information based on requiring at least 50% detection.
    """
    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_dir + "{ID}-genes.fasta"
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        contig_cov_and_det_tmp = mapping_dir + "{ID}-contig-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp",
        contig_cov_tmp = mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv",
        contig_covs = mapping_dir + "{ID}-contig-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.contig_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # contigs
        grep -v "#" {params.contig_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.contig_cov_tmp}
        cat <( printf "contig_ID\tcoverage\n" ) {params.contig_cov_tmp} > {output.contig_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    """
    This rule builds the bowtie2 index and runs the mapping for each sample.
    """
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    params:
        index = mapping_dir + "{ID}-index"
    output:
        mapping_dir + "{ID}.bam"
    log:
        mapping_dir + "{ID}-mapping.log"
    shell:
        """
        bowtie2-build {input.assembly} {params.index} > /dev/null 2>&1
        bowtie2 --threads {num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} 2> {log} | samtools view -b | samtools sort -@ {num_threads} > {output} 2> /dev/null
        samtools index -@ {num_threads} {output}

        rm {params.index}*
        """


rule run_tax_classification:
    """
    This rule runs the gene- and contig-level taxonomic classifications for each sample.
    """
    input:
        assembly = assemblies_dir + "{ID}-assembly.fasta",
        AA = genes_dir + "{ID}-genes.faa"
    output:
        gene_tax_out = annotations_and_tax_dir + "{ID}-gene-tax.tsv",
        contig_tax_out = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        CAT_DB = CAT_DIR + "/2020-06-18_database/",
        CAT_TAX = CAT_DIR + "/2020-06-18_taxonomy/",
        tmp_out_prefix = annotations_and_tax_dir + "{ID}-tax-out.tmp",
        tmp_genes = annotations_and_tax_dir + "{ID}-gene-tax.tmp",
        tmp_contigs = annotations_and_tax_dir + "{ID}-contig-tax.tmp"
    shell:
        """
        CAT contigs -d {params.CAT_DB} -t {params.CAT_TAX} -n {num_cpus} -r 3 --top 4 --I_know_what_Im_doing -c {input.assembly} -p {input.AA} -o {params.tmp_out_prefix} > /dev/null 2>&1

        # adding names to gene classifications
        CAT add_names -i {params.tmp_out_prefix}.ORF2LCA.txt -o {params.tmp_genes} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting gene classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "lineage" ) {{ print $1,$2,$4,$5,$6,$7,$8,$9,$10 }} \
        else if ( $2 == "ORF has no hit to database" || $2 ~ /^no taxid found/ ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($2,lineage,";"); print $1,lineage[n],$4,$5,$6,$7,$8,$9,$10 }} }} ' {params.tmp_genes} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/^# ORF/gene_ID/' | sed 's/lineage/taxid/' | \
        sed 's/\*//g' > {output.gene_tax_out}

        # adding names to contig classifications
        CAT add_names -i {params.tmp_out_prefix}.contig2classification.txt -o {params.tmp_contigs} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting contig classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "classification" ) {{ print $1,$4,$6,$7,$8,$9,$10,$11,$12 }} \
        else if ( $2 == "unclassified" ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($4,lineage,";"); print $1,lineage[n],$6,$7,$8,$9,$10,$11,$12 }} }} ' {params.tmp_contigs} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/: [0-9\.]*//g' | sed 's/^# contig/contig_ID/' | \
        sed 's/lineage/taxid/' | sed 's/\*//g' > {output.contig_tax_out}

        rm -rf {annotations_and_tax_dir}{wildcards.ID}*tmp*
        """


rule run_KO_annotation:
    """
    This rule runs the gene-level (KO) functional annotation for each sample.
    """
    input:
        genes_dir + "{ID}-genes.faa"
    output:
        annotations_and_tax_dir + "{ID}-annotations.tsv"
    params:
        KO_profiles = KO_DIR + "/profiles/",
        KO_list = KO_DIR + "/ko_list",
        tmp_out = annotations_and_tax_dir + "{ID}-KO-tab.tmp",
        tmp_dir = annotations_and_tax_dir + "{ID}-tmp-KO-dir"
    shell:
        """
        exec_annotation -p {params.KO_profiles} -k {params.KO_list} --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 

        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}

        rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule call_genes:
    """
    This rule calls genes on each sample's assembly file.
    """
    input:
        assemblies_dir + "{ID}-assembly.fasta"
    output:
        AA = genes_dir + "{ID}-genes.faa",
        nt = genes_dir + "{ID}-genes.fasta",
        gff = genes_dir + "{ID}-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule summarize_assemblies:
    """
    This rule summarizes and reports general stats for all individual sample assemblies in one table.
    """
    input:
        expand(assemblies_dir + "{ID}-assembly.fasta", ID = sample_ID_list),
    output:
        assemblies_dir + "assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly -o {output} {input}
        """


rule assemble:
    """
    This rule handles running the assembly for each individual sample.
    """
    input:
        R1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        assemblies_dir + "{ID}-assembly.fasta"
    log:
        assemblies_dir + "{ID}-assembly.log"
    shell:
        """
        megahit -1 {input.R1} -2 {input.R1} -m {max_mem} -t {num_threads} -o {assemblies_dir}{wildcards.ID}-megahit-out > {log} 2>&1
        bit-rename-fasta-headers -i {assemblies_dir}{wildcards.ID}-megahit-out/final.contigs.fa -w c_{wildcards.ID} -o {output}

        rm -rf {assemblies_dir}{wildcards.ID}-megahit-out/
        """


rule raw_fastqc:
    """
    This rule runs fastqc on all raw input fastq files.
    """
    input:
        raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix        
    output:
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix,
        raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule raw_multiqc:
    """
    This rule collates all raw fastqc outputs.
    """
    input:
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R1_suffix, ID = sample_ID_list),
        expand(raw_reads_dir + filename_prefix + "{ID}" + raw_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "raw_multiqc_report.html.zip",
        fastqc_out_dir + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}raw_tmp_qc {raw_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_data.zip {fastqc_out_dir}raw_multiqc_data.zip
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_report.html {fastqc_out_dir}raw_multiqc_report.html
        zip -q {fastqc_out_dir}raw_multiqc_report.html.zip {fastqc_out_dir}raw_multiqc_report.html && rm {fastqc_out_dir}raw_multiqc_report.html
          # removing the individual fastqc files
        rm -rf {raw_reads_dir}*fastqc* {fastqc_out_dir}raw_tmp_qc/
        """


rule bbduk:
    """
    This rule runs quality filtering/trimming on raw input fastq files for each individual sample.
    """
    input:
        in1 = raw_reads_dir + filename_prefix + "{ID}" + filename_R1_suffix,
        in2 = raw_reads_dir + filename_prefix + "{ID}" + filename_R2_suffix
    output:
        out1 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        out2 = filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    log:
        filtered_reads_dir + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l k=17 ftm=5 qtrim=rl \
                trimq=10 mlf=0.5 maxns=0 > {log} 2>&1
        """


rule trimmed_fastqc:
    """
    This rule runs fastqc on all trimmed/filtered input fastq files.
    """
    input:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_filename_R2_suffix
    output:
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix,
        filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule trimmed_multiqc:
    """
    This rule collates all trimmed/filtered fastqc outputs.
    """
    input:
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R1_suffix, ID = sample_ID_list),
        expand(filtered_reads_dir + filename_prefix + "{ID}" + trimmed_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "trimmed_multiqc_report.html.zip",
        fastqc_out_dir + "trimmed_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}trimmed_tmp_qc {filtered_reads_dir} > /dev/null 2>&1
          # renaming the outputs and zipping html to enable uploading in GL framework
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_data.zip {fastqc_out_dir}trimmed_multiqc_data.zip
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_report.html {fastqc_out_dir}trimmed_multiqc_report.html
        zip -q {fastqc_out_dir}trimmed_multiqc_report.html.zip {fastqc_out_dir}trimmed_multiqc_report.html && rm {fastqc_out_dir}trimmed_multiqc_report.html
          # removing the individual fastqc files and temp locations
        rm -rf {filtered_reads_dir}*fastqc* {fastqc_out_dir}trimmed_tmp_qc/
        """


rule clean_all:
    shell:
        "rm -rf {fastqc_out_dir} {filtered_reads_dir} {assembly_based_dir} {read_based_dir} .snakemake/"


rule clean_some:
    shell:
        "rm -rf {fastqc_out_dir} {genes_dir} {mapping_dir} {combined_output_dir} {assemblies_dir}assembly-summaries.tsv"
